---
description: 
globs: test_*.py,conftest.py,run_all_tests.sh
alwaysApply: false
---
# Testing Standards for pyERP

## Overview
This document outlines comprehensive testing standards and practices for the pyERP system. These guidelines ensure that all components maintain high quality, reliability, and security through effective testing methodologies. By following these standards, we can catch bugs early, ensure consistent behavior across modules, and maintain a robust enterprise resource planning solution.

## Project Test Structure

### Business Module Organization
Tests in pyERP follow a modular organization that mirrors the business domain structure. Each business module contains its own dedicated test directory with specialized test files for different testing purposes. This organization facilitates focused testing, improves maintainability, and allows developers to quickly locate relevant tests.

The cross-module tests directory contains integration tests that verify interactions between different business modules, ensuring that components work together correctly. Core tests focus on the fundamental infrastructure of the system, while legacy API tests verify proper integration with external systems.

### Test File Organization
Each test file should focus on a specific component or functionality aspect. Test files should begin with clear docstrings explaining their purpose and the components they test. Import statements should be organized by standard libraries first, followed by Django/framework imports, and finally project-specific imports.

Test classes should be descriptive and contain focused methods for specific test scenarios. Every test method requires a clear docstring explaining what is being tested and the expected outcome. The setUp and tearDown methods should establish necessary test data and properly clean resources after tests.

## Test Categories and Patterns

### 1. Service Tests
Service tests verify business logic in service layers without mocking dependencies when practical. These tests interact with real database connections to ensure all components work correctly together. They focus on validating the business rules, calculations, validations, and other core functionality.

Service tests should create all necessary data during setup, execute service methods with appropriate parameters, and validate both return values and database state changes. They should cover various scenarios including edge cases and ensure business rules are enforced correctly.

### 2. Mocked Service Tests
Mock-based service tests isolate specific components by replacing dependencies with controlled test doubles. This approach allows testing a service's internal logic without relying on external systems or databases, resulting in faster test execution and more precise failure diagnosis.

These tests should focus on verifying internal service behavior like correct method calls, parameter validation, and proper error propagation. Mock configuration should be established in setUp methods or custom fixtures, with assertions validating both the return values and the expected interactions with dependencies.

### 3. View Tests
View tests validate API endpoints and view functions, ensuring they handle requests correctly, apply appropriate permissions, and return proper responses. These tests use Django's testing framework and REST framework's APIClient to simulate HTTP requests and verify responses.

View tests should create necessary test data, authenticate if required, execute API calls with appropriate parameters, and validate both the response status code and content. They should verify that database state changes occur correctly and that proper error responses are generated for invalid inputs.

### 4. Error Handling Tests
Error handling tests specifically target exceptional conditions to verify that errors are properly caught, logged, and reported. They ensure that appropriate error responses are returned rather than exposing internal error details or causing system failures.

These tests should verify that each potential error condition is properly handled, including missing parameters, invalid inputs, database constraints, and service exceptions. They should confirm that proper HTTP status codes are returned, error messages are understandable, and sensitive details are not exposed.

### 5. Model Tests
Model tests verify that Django models function correctly, including field validations, constraints, calculated properties, and model methods. They ensure data integrity at the database level and validate business rules implemented in model methods.

These tests should create instances with various attribute combinations, verify that validation constraints are enforced, test computed properties, and confirm that model methods produce expected results. They should also verify that signals and hooks work correctly during model lifecycle events.

### 6. Legacy API Tests
Legacy API tests validate integration with external systems, particularly the legacy 4D API. These tests verify that the API client correctly handles authentication, request formatting, response parsing, error handling, and connection management.

Since external systems should not be directly accessed during testing, these tests heavily rely on mocking to simulate API responses. They should verify correct request formation, authentication flows, response handling, and error recovery mechanisms.

## Testing Best Practices

### 1. Use Fixtures for Common Test Data
Pytest fixtures provide a powerful mechanism for reusable test setup. Fixtures should be used for common test data like standard model instances, authentication, and environment configuration. They improve test maintainability by centralizing setup logic and reducing duplication.

Module-specific fixtures should be defined in each module's conftest.py file, while global fixtures belong in the root conftest.py. Fixtures should be focused, properly documented, and designed to be combinable to support various test scenarios.

### 2. Mock External Dependencies
External dependencies including APIs, services, and resource-intensive components should be consistently mocked during testing. This prevents tests from being affected by external system availability and ensures consistent test behavior.

Mocks should be configured to return realistic data that closely resembles production behavior. Common mocking patterns should be standardized across the codebase, preferably in shared helper modules, to ensure consistency and reduce duplication.

### 3. Test Both Happy Path and Error Cases
Comprehensive testing requires covering both successful operations and error conditions. Every feature should have tests that verify normal operation as well as how it handles various error conditions, invalid inputs, and edge cases.

Error case tests should verify that appropriate error messages are returned, proper status codes are used, and that the system remains in a consistent state after error conditions. They should also ensure that errors are properly logged for diagnostic purposes.

### 4. Use Proper Test Isolation
Tests must be independent and not affect each other's execution. Each test should set up its own data, execute operations in isolation, and clean up afterward. Global state modification should be avoided, and if necessary, properly reset between tests.

Database transactions should be properly managed to prevent test data from leaking between tests. Time-dependent tests should use controlled time mocking rather than depending on real system time. Resource cleanup should occur reliably even when tests fail.

### 5. Use Descriptive Test Names
Test method names should clearly describe what they're testing and the expected outcome. Follow the pattern of test_[function/method]_[scenario]_[expected_result] to provide immediate understanding of test purpose.

Names should be specific enough to identify the exact scenario being tested while remaining reasonably concise. When test failures occur, the name alone should provide significant insight into what functionality has broken.

## Test Coverage

### 1. Coverage Goals
High test coverage is essential for critical system components. We aim for at least 80% code coverage for all business modules, with critical paths reaching 100% coverage. Core services, error handling, and security-sensitive components require complete coverage.

Coverage goals should be balanced with practicality. Focus on thorough testing of business-critical paths first, then expand coverage to less critical areas. Complex decision logic and error handling deserve special attention to ensure all branches are tested.

### 2. Running Coverage Reports
Coverage tools like pytest-cov provide detailed insights into which code paths are executed during tests. Regular coverage analysis helps identify undertested areas and guides further test development. 

Coverage reports should be generated in multiple formats including console output for quick reference and HTML reports for detailed analysis. These reports should be reviewed regularly as part of the development process to ensure testing remains comprehensive as code evolves.

### 3. Continuous Integration
Automated testing in CI/CD pipelines ensures consistent validation across all changes. The pipeline should run the complete test suite on every commit, with failures preventing code from progressing to production.

Coverage trends should be tracked over time to prevent deterioration. The CI system should enforce minimum coverage thresholds for critical modules and alert teams when coverage decreases significantly. Test results should be easily accessible to all team members.

## Testing Environment

### 1. Test Database
Tests should use an isolated database environment that is separate from development and production. For most tests, an in-memory SQLite database provides the fastest execution while maintaining compatibility with Django's ORM.

The test database should be created fresh for each test run and destroyed afterward to prevent test data persistence issues. Database migrations should be applied to ensure the schema matches production, but with performance optimizations when possible.

### 2. Environment Variables
Environmental configuration for testing should override external service endpoints, authentication credentials, and other production settings. These overrides should redirect external connections to local mocks or test instances.

Configuration should be implemented in a way that prevents accidental connections to production systems during testing. Test-specific settings should be clearly marked and isolated from production configuration to prevent confusion.

### 3. Test Logging
Logging during tests should be configured to capture diagnostic information without overwhelming output. Log messages should be directed to appropriate handlers based on severity, with critical errors visible in test output and debug messages available for troubleshooting.

Log formatting should include timestamps, log levels, and source information to facilitate problem diagnosis. In CI environments, logs should be preserved for failed tests to aid in diagnosing intermittent issues.

## Running Tests

### 1. Running with Django Test Command
Django's test command provides a familiar interface for running tests in the context of a Django project. It handles database setup/teardown and integrates with Django's settings management.

Developers should become familiar with filtering options to run specific test subsets during development. These commands can target specific modules, classes, or methods to enable focused testing during feature development.

### 2. Running with pytest
Pytest offers advanced testing features including better fixture support, more detailed failure reports, and enhanced test discovery. It integrates well with Django tests while providing additional capabilities.

Parameter-based test selection allows running focused test subsets matching particular patterns or markers. Verbose output options provide detailed information about test execution, while integrations with debuggers and other tools enhance the testing workflow.

### 3. Test Debugging
When tests fail, effective debugging tools help quickly identify root causes. Interactive debugging with pdb allows examining the exact state at failure points, while detailed traceback options provide context for failures.

Print output can be enabled to see runtime values during test execution. Local variable inspection helps understand the test environment at failure points. These debugging approaches should be used systematically to efficiently diagnose test failures.

## JavaScript Testing

### 1. Jest Testing Framework
Jest provides a comprehensive testing solution for JavaScript code, including both browser and Node.js environments. Its integrated approach combines test running, assertion library, mocking capabilities, and coverage reporting in a single framework.

Jest's snapshot testing features are particularly valuable for UI components, allowing detection of unexpected changes in rendered output. Its watch mode supports rapid development cycles by automatically re-running relevant tests when files change.

### 2. Component Testing
React components require specialized testing approaches that focus on user interactions rather than implementation details. React Testing Library promotes testing components as users would interact with them, focusing on accessibility and visible behavior.

Component tests should verify proper rendering, state management, event handling, and asynchronous operations. They should simulate real user interactions like clicks and form submissions rather than directly manipulating component internals.

### 3. Jest Configuration
Jest configuration should be tailored to the project's specific needs, including proper module resolution, file transformations, and environment setup. The jest.config.js file centralizes these settings and ensures consistent test behavior.

Configuration should establish appropriate coverage thresholds, customize reporters, and define mock behaviors for external dependencies. Test setup files should prepare the environment with necessary globals and utilities.

### 4. Running Jest Tests
Standard Jest commands should be documented and used consistently across the team. Developers should be familiar with running complete test suites, specific test files, or filtered test sets based on patterns.

Watch mode should be used during active development to get immediate feedback on changes. Coverage reporting should be integrated into the workflow to ensure JavaScript code maintains high test coverage.

### 5. Integration with Django
Projects combining Django backend with JavaScript frontend require specific integration strategies. Test organization should mirror the frontend architecture while maintaining clear separation from backend tests.

Combined coverage reporting provides a complete view of project test status across both backend and frontend components. The test environment should be configured to simulate the production integration between Django and JavaScript components.

## Mutation Testing

### 1. Python Mutation Testing with Mutmut
Mutation testing introduces small changes (mutations) to the codebase and verifies that tests detect these changes. Mutmut automates this process for Python code, helping identify weaknesses in test assertions and coverage.

Mutations include changing operators, modifying boolean conditions, altering return values, and removing code. Tests should fail when relevant code is mutated, indicating that they effectively verify the expected behavior.

### 2. JavaScript Mutation Testing with Stryker
Stryker Mutator provides similar capabilities for JavaScript code, with support for various frameworks including Jest and React Testing Library. It offers comprehensive reports that highlight surviving mutations requiring additional test coverage.

Mutation testing should be applied to critical business logic first, as it requires significant computational resources. Results should guide the development of more robust test assertions rather than simply increasing redundant test coverage.

### 3. Interpreting Mutation Results
Mutation testing produces a mutation score representing the percentage of mutations detected by tests. Higher scores indicate more effective test suites that thoroughly verify code behavior rather than simply executing code paths.

Analysis should focus on patterns in surviving mutations, which often reveal assumptions in the code not being verified by tests. Correlation between traditional coverage metrics and mutation scores provides insight into test quality beyond simple execution.

### 4. Mutation Testing Best Practices
Effective mutation testing requires strategic application to maximize value while managing computational costs. Starting with small, critical modules allows focused improvement of test suites for the most important code.

Focus on business logic that directly impacts system correctness rather than infrastructural or UI code. Integrate mutation testing in CI/CD pipelines on a scheduled basis to monitor test quality over time. Regular review of surviving mutations guides continuous test improvement.

### 5. Integrating with CI/CD
Mutation testing in CI/CD pipelines requires careful implementation to balance thoroughness with performance. Scheduled execution rather than running on every commit prevents excessive build times while still monitoring test quality.

Reports should be automatically generated and archived to track trends over time. Score thresholds can be established to alert teams when test effectiveness decreases. Performance optimization techniques like test selection and parallelization maximize efficiency.

## Security Testing

### 1. Python Security Testing with Bandit
Bandit performs static analysis of Python code to identify common security vulnerabilities. It detects issues like injection vulnerabilities, insecure cryptography, and hardcoded credentials through pattern matching and AST analysis.

Configuration should be tailored to the project's specific security requirements, with custom rules for organization-specific concerns. Severity levels help prioritize remediation efforts based on potential impact and exploitation likelihood.

### 2. JavaScript Security Testing with ESLint Security Plugin
ESLint security plugins provide similar capabilities for JavaScript code, detecting vulnerabilities in frontend and Node.js components. Rules cover issues like XSS vulnerabilities, unsafe resource loading, and injection risks.

Custom rule configuration allows adapting security checks to the specific frameworks and patterns used in the project. Integration with IDEs provides immediate feedback during development, while automated scanning ensures comprehensive coverage.

### 3. SAST Integration in CI/CD
Security Static Application Testing should be fully integrated into CI/CD workflows to ensure consistent application. Automated scans on every commit provide immediate feedback on potential security issues introduced by changes.

Report generation should include details needed for remediation while being accessible to developers. Issue prioritization based on severity guides efficient resolution efforts. Blocking criteria for critical vulnerabilities prevents introducing serious security flaws.

### 4. Security Testing Best Practices
Effective security testing combines automated checks with manual review and specialized testing approaches. Automated checks should run continuously to catch common issues, while periodic manual reviews address more complex vulnerabilities.

Define a security baseline with minimum standards for all code. Prioritize fixing critical issues immediately, with defined timeframes for addressing less severe findings. Regularly scan dependencies for published vulnerabilities and update as needed.

### 5. Vulnerability Management
When security issues are identified, a structured process ensures proper handling and resolution. Issue tracking should record all vulnerabilities with severity assessments and remediation plans.

Risk assessment helps prioritize fixes based on potential impact and exploitation likelihood. Remediation plans should include clear steps and verification testing to ensure issues are fully resolved. Documentation of security findings and resolutions creates an institutional knowledge base.

## Testing Entrypoint

### Main Testing Script
PyERP provides a centralized testing entrypoint through the `run_all_tests.sh` script. This script serves as the primary interface for executing various types of tests across the system, offering flexibility through command-line options while maintaining consistency in test execution.

### Usage and Options
The script supports multiple testing modes and configurations:

- **Test Types**: Run specific categories of tests using the `-t/--type` option:
  - `all`: Execute the complete test suite
  - `unit`: Run only unit tests
  - `backend`: Run backend-specific tests
  - `ui`: Run UI and frontend tests
  - Other specialized test categories as needed

- **Coverage Analysis**: Control coverage reporting:
  - Coverage is enabled by default
  - `-n/--no-coverage`: Disable coverage reporting
  - `-c/--coverage`: Explicitly enable coverage (default behavior)

- **Advanced Testing Methods**:
  - `-m/--mutation`: Enable mutation testing to identify weaknesses in test assertions
  - `-f/--fuzz`: Run fuzz testing to detect vulnerabilities with random inputs
  - `-s/--security`: Execute security-focused tests using tools like Bandit

- **Output Control**:
  - `-v/--verbose`: Increase output verbosity for detailed test information
  - `-q/--quiet`: Reduce output to essential information only

### Example Usage Patterns
The testing script accommodates various testing scenarios:

```bash
# Run all tests with default options
./run_all_tests.sh

# Run unit tests with mutation testing
./run_all_tests.sh -t unit -m

# Run backend tests with detailed output
./run_all_tests.sh -t backend -v

# Run security-focused tests only
./run_all_tests.sh -s

# Run complete test suite with all validation types
./run_all_tests.sh -t all -m -f -s
```

### Integration with CI/CD
The testing entrypoint is designed to work seamlessly in both local development environments and CI/CD pipelines. In CI environments, the script can be invoked with appropriate options to execute relevant test suites based on the pipeline stage and branch context.

This standardized approach to test execution ensures consistency between local and CI environments, reducing the "works on my machine" problem and providing reliable validation across all stages of development.

### Extending the Test Entrypoint
When adding new test categories or testing tools to the system, extend the main testing script rather than creating separate execution paths. This maintains the single entrypoint pattern and ensures all testing capabilities remain accessible through the consistent interface.

New test types should be added to the help documentation and implemented in the underlying test execution scripts that are called by the main entrypoint.